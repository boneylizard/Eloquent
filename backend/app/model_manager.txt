import os
import logging
import asyncio
import glob
import requests
from typing import Dict, Any, Optional
from pathlib import Path

# --- Dependency Checks ---
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False

try:
    from ctransformers import AutoModelForCausalLM
    CTRANSFORMERS_AVAILABLE = True
except ImportError:
    CTRANSFORMERS_AVAILABLE = False

MODEL_DIR = r"C:\Users\bpfit\OneDrive\Desktop\LLM AI GGUFs"  # Your model directory

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ModelManager:
    def __init__(self):
        self.loaded_models: Dict[str, Dict[str, Any]] = {}  # Stores model objects with GPU info
        self.lock = asyncio.Lock()
        self.models_dir = Path(MODEL_DIR)
        self.has_gpu = self._detect_gpu()
        self.gpu_info = self._get_gpu_info()
        
        # Set global environment variables
        os.environ["GGML_VERBOSE"] = "1"
        
        # Get GPU ID from environment if available (for multi-process mode)
        self.default_gpu_id = int(os.environ.get("GPU_ID", 0))
        logging.info(f"Default GPU ID set to: {self.default_gpu_id}")
        
    def _detect_gpu(self) -> bool:
        try:
            import torch
            gpu_available = torch.cuda.is_available()
            logging.info(f"GPU detected: {gpu_available}")
            return gpu_available
        except ImportError:
            logging.warning("PyTorch not found, assuming no GPU.")
            return False
            
    def _get_gpu_info(self) -> Dict[str, Any]:
        """Gather information about available GPUs"""
        gpu_info = {
            "count": 0,
            "names": [],
            "cuda_version": None,
            "memory": []
        }
        
        try:
            import torch
            if torch.cuda.is_available():
                gpu_info["count"] = torch.cuda.device_count()
                gpu_info["cuda_version"] = torch.version.cuda
                
                for i in range(gpu_info["count"]):
                    gpu_info["names"].append(torch.cuda.get_device_name(i))
                    # Try to get memory info
                    try:
                        mem_free, mem_total = torch.cuda.mem_get_info(i)
                        gpu_info["memory"].append({
                            "free_mb": mem_free / (1024 * 1024),
                            "total_mb": mem_total / (1024 * 1024)
                        })
                    except:
                        gpu_info["memory"].append({"free_mb": 0, "total_mb": 0})
                    
                logging.info(f"Found {gpu_info['count']} GPUs: {', '.join(gpu_info['names'])}")
                logging.info(f"CUDA version: {gpu_info['cuda_version']}")
            
        except ImportError:
            pass
            
        return gpu_info

    def _get_gpu_params(self, gpu_id: int, context_length: int = 4096) -> Dict[str, Any]:
        """Return optimal parameters for full utilization of a specific GPU"""
        params = {
            "n_ctx": context_length,        # Context size from parameter
            "n_batch": 512,                 # Batch size for prompt processing
            "n_threads": 8,                 # CPU threads to use
            "use_mmap": False,              # Use memory-mapped IO
            "use_mlock": True,              # Lock memory to prevent swapping
            "verbose": True,                # Enable verbose output
            "f16_kv": True,                 # Use half-precision for KV cache
            "seed": 42,                     # Random seed for reproducibility
            "n_gpu_layers": 999,            # Force ALL layers onto GPU (large value > any model)
            "main_gpu": gpu_id,             # Set which GPU to use
            "offload_kqv": False,           # Disable KQV offloading to CPU
        }
        return params

        # Set CUDA_VISIBLE_DEVICES to isolate this GPU
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        logging.info(f"⭐ Isolating to GPU {gpu_id} via CUDA_VISIBLE_DEVICES={gpu_id}")
        
        # Disable tensor splits - force model to use only the specified GPU
        if self.gpu_info["count"] > 1:
            tensor_split = [0.0] * self.gpu_info["count"]
            tensor_split[gpu_id] = 1.0
            params["tensor_split"] = tensor_split
            logging.info(f"⭐ Setting tensor_split to {tensor_split} to isolate model to GPU {gpu_id}")
        
        # Set GGML environment variables to force GPU usage
        os.environ["GGML_CUDA_FORCE_MMQ"] = "0"  # Disable mixed-mode quantization
        
        # Try to set the CUDA device using PyTorch as well
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.set_device(gpu_id)
                logging.info(f"⭐ PyTorch CUDA device set to {gpu_id}")
                # Log memory status
                mem_free, mem_total = torch.cuda.mem_get_info(gpu_id)
                logging.info(f"GPU {gpu_id} memory: {mem_free/(1024*1024):.0f}MB free, {mem_total/(1024*1024):.0f}MB total")
        except Exception as e:
            logging.warning(f"Could not set PyTorch CUDA device: {e}")
            
        return params

    async def _load_with_llama_cpp(self, model_path: str, gpu_id: Optional[int] = None, n_ctx: Optional[int] = 4096, **kwargs):
        """Load a model using the llama-cpp-python library"""
        try:
            logging.info(f"⭐ Loading model on GPU {gpu_id}: {model_path}")
            loop = asyncio.get_running_loop()
            
            # Get optimal parameters for this specific GPU
            model_params = self._get_gpu_params(gpu_id, context_length=n_ctx)

            
            # Override with any user-provided parameters
            model_params.update(kwargs)
            
            # Start timing
            import time
            start_time = time.time()
            
            logging.info(f"Starting model load with parameters: {model_params}")
            
            # Load the model
            model = await loop.run_in_executor(None, lambda: Llama(
                model_path=str(model_path),
                **model_params
            ))


            # Add diagnostic info after loading
            end_time = time.time()
            load_time = end_time - start_time
            model_size_mb = Path(model_path).stat().st_size / (1024 * 1024)
            
            logging.info(f"⭐ Model loaded in {load_time:.2f} seconds")
            logging.info(f"⭐ Model file size: {model_size_mb:.2f} MB")
            
            # Try to get VRAM usage after loading
            try:
                import torch
                if torch.cuda.is_available():
                    mem_free, mem_total = torch.cuda.mem_get_info(gpu_id)
                    mem_used = mem_total - mem_free
                    logging.info(f"⭐ GPU {gpu_id} memory used: {mem_used/(1024*1024):.2f}MB of {mem_total/(1024*1024):.2f}MB")
            except:
                pass
                
            logging.info(f"✅ Successfully loaded model on GPU {gpu_id}: {model_path}")
            return model
        except Exception as e:
            logging.exception(f"❌ Error loading model with llama-cpp: {e}")
            raise

    def _load_with_ctransformers(self, model_path: str, gpu_id: Optional[int] = None, **kwargs):
        """Load a model using the ctransformers library"""
        try:
            logging.info(f"Loading with ctransformers: {model_path} on GPU {gpu_id}")
            
            # For ctransformers, handle GPU selection
            if gpu_id is not None and gpu_id >= 0:
                kwargs["device"] = f"cuda:{gpu_id}"
                
            model = AutoModelForCausalLM.from_pretrained(model_path, **kwargs)
            logging.info(f"Successfully loaded model with ctransformers: {model_path} on GPU {gpu_id}")
            return model
        except Exception as e:
            logging.exception(f"Error loading model with ctransformers: {e}")
            raise

    async def load_model(
        self,
        model_name: str,
        gpu_id: Optional[int] = None,
        model_path: Optional[str] = None,
        context_length: Optional[int] = 4096,
        **kwargs
    ) -> None:
        """
        Load a model onto a specific GPU

        Args:
            model_name: Name of the model file
            gpu_id: GPU ID to load onto (0, 1, etc.), or None to use default
            model_path: Optional explicit path to model file
            context_length: The context length to use for this model (n_ctx)
            **kwargs: Additional parameters to pass to the model loader
        """
        async with self.lock:
            # Use the default GPU ID if none specified
            if gpu_id is None:
                gpu_id = self.default_gpu_id

            # Create a unique key for this model
            model_key = f"{model_name}"

            if model_key in self.loaded_models:
                current_gpu = self.loaded_models[model_key].get("gpu_id")
                if current_gpu == gpu_id and self.loaded_models[model_key].get("n_ctx") == context_length:
                    logging.info(f"Model {model_name} already loaded on GPU {gpu_id} with context length {context_length}.")
                    return
                else:
                    # Model is loaded but on a different GPU or with a different context length, unload it first
                    logging.info(f"Model {model_name} already loaded on GPU {current_gpu} with context length {self.loaded_models[model_key].get('n_ctx')}, unloading first.")
                    await self.unload_model(model_name)

            # If model_path not provided, search for it
            if not model_path:
                model_files = glob.glob(os.path.join(self.models_dir, "**", model_name), recursive=True)
                if not model_files:
                    raise FileNotFoundError(f"Model {model_name} not found in {self.models_dir}")
                model_path = model_files[0]

            logging.info(f"⭐ Loading model {model_name} from {model_path} on GPU {gpu_id} with context length {context_length}")

            try:
                if not self.has_gpu:
                    logging.warning("No GPU detected, model will use CPU only")
                    kwargs["n_gpu_layers"] = 0
                    gpu_id = -1
                elif gpu_id is not None and gpu_id >= 0:
                    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)  # Ensure this is set *before* loading
                    logging.info(f"⭐ Setting CUDA_VISIBLE_DEVICES to {gpu_id} for loading {model_name}")

                # Always force n_gpu_layers to a high value to ensure all layers go to GPU
                kwargs["n_gpu_layers"] = 999
                logging.info(f"⭐ Forcing ALL layers to GPU {gpu_id} with n_gpu_layers=999")

                if LLAMA_CPP_AVAILABLE:
                    # Use n_ctx here
                    model = await self._load_with_llama_cpp(str(model_path), gpu_id, n_ctx=context_length, **kwargs)

                elif CTRANSFORMERS_AVAILABLE:
                    model = self._load_with_ctransformers(str(model_path), gpu_id, **kwargs)
                else:
                    raise ImportError("No compatible GGUF model loader found.")

                # Store the model with metadata including context length (n_ctx)
                self.loaded_models[model_key] = {
                    "model": model,
                    "gpu_id": gpu_id,
                    "path": str(model_path),
                    "n_ctx": context_length  # Store n_ctx
                }

                logging.info(f"✅ Model {model_name} loaded successfully on GPU {gpu_id}.")

                # Run a quick inference test to verify model works and measure speed
                try:
                    if LLAMA_CPP_AVAILABLE:
                        import time
                        test_prompt = "Hello, this is a quick test."
                        logging.info(f"Running test inference with prompt: '{test_prompt}'")

                        start_time = time.time()
                        test_output = model(test_prompt, max_tokens=20)
                        end_time = time.time()

                        inference_time = end_time - start_time
                        output_text = test_output["choices"][0]["text"]
                        token_count = len(output_text.split())
                        tokens_per_second = token_count / inference_time if inference_time > 0 else 0

                        logging.info(f"⭐ Test inference successful: {output_text}")
                        logging.info(f"⭐ Generated {token_count} tokens in {inference_time:.2f}s ({tokens_per_second:.2f} tokens/s)")
                except Exception as test_e:
                    logging.warning(f"Test inference failed: {test_e}")

            except Exception as e:
                logging.exception(f"❌ Error loading model {model_name} on GPU {gpu_id}: {e}")
                raise

    async def unload_model(self, model_name: str):
        async with self.lock:
            model_key = f"{model_name}"
            
            if model_key in self.loaded_models:
                gpu_id = self.loaded_models[model_key].get("gpu_id", "unknown")
                del self.loaded_models[model_key]
                logging.info(f"Model {model_name} unloaded from GPU {gpu_id}.")
                import gc
                gc.collect()
                
                # Try to clear CUDA cache
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        logging.info(f"CUDA cache cleared for GPU {gpu_id}")
                except:
                    pass

    async def unload_all_models(self):
        async with self.lock:
            for model_key in list(self.loaded_models.keys()):
                await self.unload_model(model_key)
            logging.info("All models unloaded")

    def list_available_models(self):
        model_files = [filename for filename in os.listdir(MODEL_DIR) if filename.endswith(".gguf")]
        return {"available_models": model_files}

    def get_loaded_models(self):
        """Return information about loaded models including which GPU they're on"""
        models_info = []
        for model_name, model_data in self.loaded_models.items():
            gpu_id = model_data.get("gpu_id", -1)
            gpu_name = "CPU"
            if gpu_id >= 0 and gpu_id < len(self.gpu_info["names"]):
                gpu_name = self.gpu_info["names"][gpu_id]
                
            models_info.append({
                "name": model_name,
                "gpu_id": gpu_id,
                "gpu_name": gpu_name
            })
            
        return {"loaded_models": models_info}
        
    def get_model(self, model_name: str):
        """Get the actual model object for inference"""
        model_key = f"{model_name}"
        
        if model_key not in self.loaded_models:
            raise ValueError(f"Model {model_name} is not loaded")
            
        return self.loaded_models[model_key]["model"]
        
    def get_system_info(self):
        """Returns system information including GPU details"""
        info = {
            "gpu_available": self.has_gpu,
            "gpu_count": self.gpu_info["count"],
            "gpu_names": self.gpu_info["names"],
            "cuda_version": self.gpu_info["cuda_version"],
            "loaded_models": self.get_loaded_models()["loaded_models"]
        }
        
        # Try to add current memory info for each GPU
        try:
            import torch
            if torch.cuda.is_available():
                gpu_memory = []
                for i in range(self.gpu_info["count"]):
                    mem_free, mem_total = torch.cuda.mem_get_info(i)
                    gpu_memory.append({
                        "gpu_id": i,
                        "free_mb": mem_free / (1024 * 1024),
                        "total_mb": mem_total / (1024 * 1024),
                        "used_mb": (mem_total - mem_free) / (1024 * 1024),
                        "used_percent": ((mem_total - mem_free) / mem_total) * 100
                    })
                info["gpu_memory"] = gpu_memory
        except:
            pass
        
        return info
        
    def generate(self, request):
        """
        Generate a response using the loaded model.
        This addresses the 'ModelManager' object has no attribute 'generate' error.
        
        Args:
            request: The generation request object containing prompt or messages
            
        Returns:
            A response object with the generated content in a consistent format
        """
        logging.info("Starting generate method")
        
        # Extract the prompt from different request formats
        prompt = None
        if hasattr(request, 'prompt'):
            prompt = request.prompt
        elif hasattr(request, 'messages') and len(request.messages) > 0:
            prompt = request.messages[-1].content
        else:
            # Try to handle dictionary format
            if isinstance(request, dict):
                prompt = request.get('prompt', '')
                if not prompt and 'messages' in request and len(request['messages']) > 0:
                    prompt = request['messages'][-1].get('content', '')
        
        if not prompt:
            logging.warning("Could not extract prompt from request")
            prompt = ""
            
        logging.info(f"Extracted prompt: {prompt[:50]}...")
        
        # Check for memory curation keywords in the prompt
        memory_keywords = ["clean up the memory", "remove duplicates", "curate memory"]
        should_curate = any(keyword in prompt.lower() for keyword in memory_keywords)
        
        # Execute memory curation if needed
        curation_result = ""
        if should_curate:
            logging.info("Memory curation keywords detected, calling curation endpoint")
            try:
                # Use synchronous request to avoid awaiting a dict
                response = requests.post("http://localhost:8001/memory/curate")
                if response.status_code == 200:
                    curation_result = "Memory curation complete. "
                    logging.info("Memory curation successful")
                else:
                    curation_result = f"Memory curation failed with status {response.status_code}. "
                    logging.warning(f"Memory curation failed: {response.status_code}")
            except Exception as e:
                curation_result = f"Memory curation error: {str(e)}. "
                logging.error(f"Error during memory curation: {e}")
            
            # For memory curation requests, we still want to respond with a proper message
            if hasattr(request, 'model_name'):
                model_name = request.model_name
            elif isinstance(request, dict) and 'model_name' in request:
                model_name = request.get('model_name')
            else:
                model_name = "default_model"
                
            # Return in a format matching what your frontend expects
            return {
                "text": f"🧠 {curation_result}I've cleaned up the memory system for you, Bernard. All duplicates have been removed and the memory store now contains only relevant, high-quality memories.",
                "model": model_name,
                "finish_reason": "stop"
            }
        
        # Get the active model - assuming the model name is in the request or using default
        model_name = None
        if hasattr(request, 'model_name'):
            model_name = request.model_name
        elif hasattr(request, 'model'):
            model_name = request.model
        elif isinstance(request, dict):
            model_name = request.get('model_name', request.get('model'))
        
        # If no model specified, use the first loaded model
        if not model_name and self.loaded_models:
            model_name = list(self.loaded_models.keys())[0]
            logging.info(f"No model specified, using first loaded model: {model_name}")
        
        if not model_name:
            raise ValueError("No model specified and no models loaded")
        
        # Get the model object
        model = self.get_model(model_name)
        
        # Adapt request for llama-cpp vs ctransformers
        try:
            # Set parameters suitable for the model
            max_tokens = 2048  # Default
            if hasattr(request, 'max_tokens'):
                max_tokens = request.max_tokens
            elif isinstance(request, dict):
                max_tokens = request.get('max_tokens', 2048)
            
            temperature = 0.7  # Default
            if hasattr(request, 'temperature'):
                temperature = request.temperature
            elif isinstance(request, dict):
                temperature = request.get('temperature', 0.7)
            
            # Generate response with llama-cpp
            if LLAMA_CPP_AVAILABLE:
                logging.info(f"Generating with llama-cpp, max_tokens={max_tokens}, temp={temperature}")
                output = model(
                    prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stop=["User:", "Human:", "<|im_end|>"]
                )
                
                # Format the llama-cpp output to a standardized response
                result = {
                    "text": output["choices"][0]["text"],
                    "model": model_name,
                    "finish_reason": output["choices"][0]["finish_reason"]
                }
            # Generate response with ctransformers
            elif CTRANSFORMERS_AVAILABLE:
                logging.info(f"Generating with ctransformers, max_tokens={max_tokens}")
                output_text = model(prompt, max_new_tokens=max_tokens, temperature=temperature)
                result = {
                    "text": output_text,
                    "model": model_name,
                    "finish_reason": "length"
                }
            else:
                raise ImportError("No compatible model loader available")
                
            # If we did memory curation, prepend that info to the response
            if should_curate:
                logging.info(f"Adding curation result to output: {curation_result}")
                result["text"] = curation_result + result["text"]
                
            logging.info(f"Generation complete, output: {result['text'][:50]}...")
            return result
            
        except Exception as e:
            logging.exception(f"Error during generation: {e}")
            raise