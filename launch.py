import os
# Disable problematic Torch optimizations for Python 3.12+
os.environ["TORCH_DYNAMO_DISABLE"] = "1"
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import sys
import uvicorn
from multiprocessing import Process, freeze_support
import socket
import time
import webbrowser
import threading
import json
from pathlib import Path
import datetime

RUN_TAG = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_WRITE_LOCK = threading.Lock()

class TeeStream:
    """Write output to both console and a file."""
    def __init__(self, stream, file_path):
        self.stream = stream
        self.file = open(file_path, "a", encoding="utf-8")

    def write(self, message):
        self.stream.write(message)
        self.stream.flush()
        try:
            with LOG_WRITE_LOCK:
                self.file.write(message)
                self.file.flush()
        except Exception:
            pass

    def flush(self):
        self.stream.flush()
        try:
            self.file.flush()
        except Exception:
            pass

    def close(self):
        try:
            self.file.close()
        except Exception:
            pass

def get_project_root():
    """Gets the absolute path to the project's root directory (where launch.py is)."""
    return os.path.dirname(os.path.abspath(__file__))


def is_port_available(port):
    """Check if a port is available by seeing if anything is listening on it."""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(1)
            result = s.connect_ex(('127.0.0.1', port))
            # If connect succeeds (result == 0), something is listening = port NOT available
            # If connect fails (result != 0), nothing is listening = port IS available
            return result != 0
    except Exception:
        return True  # Assume available if check fails


def find_available_port(start_port, max_attempts=20):
    """Find the next available port starting from start_port."""
    for offset in range(max_attempts):
        port = start_port + offset
        if is_port_available(port):
            return port
    return None


def get_port_config():
    """Auto-find available ports, starting from defaults."""
    # Find available ports automatically
    backend_port = find_available_port(8000)
    if backend_port != 8000:
        print(f"‚ö†Ô∏è Port 8000 in use, backend will use port {backend_port}")
    
    # Secondary starts after backend port
    secondary_port = find_available_port(backend_port + 1)
    if secondary_port != 8001:
        print(f"‚ö†Ô∏è Port 8001 in use, secondary will use port {secondary_port}")
    
    # TTS starts after secondary
    tts_port = find_available_port(secondary_port + 1)
    if tts_port != 8002:
        print(f"‚ö†Ô∏è Port 8002 in use, TTS will use port {tts_port}")
    
    return {
        "backend_port": backend_port,
        "secondary_port": secondary_port,
        "tts_port": tts_port
    }


def get_local_ip():
    """Get the best candidate for local LAN IP, but print all options."""
    try:
        # Get all IPs via hostname resolution
        hostname = socket.gethostname()
        try:
            _, _, all_ips = socket.gethostbyname_ex(hostname)
            all_ips = [ip for ip in all_ips if not ip.startswith("127.")]
        except:
            all_ips = []
        
        # Also try the socket method to see what the OS thinks is the default route
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:
                s.connect(("8.8.8.8", 80))
                default_ip = s.getsockname()[0]
                if default_ip not in all_ips and not default_ip.startswith("127."):
                    all_ips.append(default_ip)
        except:
            pass

        if not all_ips:
            return "localhost"

        # Sort by preference: 192.168.x > 10.x > 172.x > others > 100.x (often VPN/CGNAT)
        def sort_key(ip):
            if ip.startswith("192.168."): return 0
            if ip.startswith("10."): return 1
            if ip.startswith("172."): return 2
            if ip.startswith("100."): return 10 # Low priority (Tailscale/CGNAT)
            return 5
            
        all_ips.sort(key=sort_key)
        best_ip = all_ips[0]
        
        if len(all_ips) > 1:
            print(f"üåç Multiple Network IPs detected: {', '.join(all_ips)}")
            print(f"üëâ Selected {best_ip} for configuration")
            
        return best_ip
            
    except Exception as e:
        print(f"‚ö†Ô∏è IP Detection failed: {e}")
        return "localhost"


def wait_for_port(host, port, timeout=60):
    """Wait until a TCP port is accepting connections."""
    start = time.time()
    while time.time() - start < timeout:
        try:
            with socket.create_connection((host, port), timeout=1):
                return True
        except OSError:
            time.sleep(0.5)
    return False


def get_log_dir(root_path):
    """Create a log directory under the project root."""
    log_dir = Path(root_path) / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    return log_dir


def get_startup_report_path(root_path):
    """Create a per-run startup report path under the project logs folder."""
    log_dir = get_log_dir(root_path)
    return log_dir / f"startup_report_{RUN_TAG}.txt"


def get_backend_log_path(port):
    """Create a per-run backend log path under the project logs folder."""
    log_dir = get_log_dir(get_project_root())
    return log_dir / f"backend_{port}_{RUN_TAG}.log"


def get_tts_log_path(port):
    """Create a per-run TTS log path under the project logs folder."""
    log_dir = get_log_dir(get_project_root())
    return log_dir / f"tts_{port}_{RUN_TAG}.log"


def append_startup_report(report_path, title, content):
    """Append a titled section to the startup report."""
    try:
        with open(report_path, "a", encoding="utf-8") as f:
            f.write(f"\n===== {title} =====\n")
            f.write(content.rstrip() + "\n")
    except Exception as e:
        print(f"WARNING: Failed to write startup report section: {e}")


def append_startup_log(log_path, message):
    """Append a line to the startup log with timestamp."""
    try:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"{timestamp} - {message}\n")
    except Exception as e:
        print(f"∆ís√ø‚Äπ,? Failed to write startup log: {e}")


def extract_startup_section(log_path, markers):
    """Extract startup section from log up to the last matching marker line."""
    log_path = Path(log_path)
    if not log_path.exists():
        return ""
    try:
        content = log_path.read_text(encoding="utf-8", errors="replace")
    except Exception:
        return ""
    last_idx = -1
    for marker in markers:
        idx = content.rfind(marker)
        if idx > last_idx:
            last_idx = idx
    if last_idx != -1:
        end_line = content.find("\n", last_idx)
        if end_line == -1:
            end_line = len(content)
        return content[:end_line].strip()
    return content.strip()


def stream_process_output(process, report_path, stop_markers=None):
    """Stream subprocess stdout to console and startup report."""
    if process.stdout is None:
        return
    stop_markers = stop_markers or []
    stop_writing = False

    def _reader():
        nonlocal stop_writing
        for line in iter(process.stdout.readline, ""):
            if not line:
                break
            try:
                sys.__stdout__.write(line)
                sys.__stdout__.flush()
            except Exception:
                pass
            if not stop_writing:
                try:
                    with LOG_WRITE_LOCK:
                        with open(report_path, "a", encoding="utf-8") as f:
                            f.write(line)
                except Exception:
                    pass
                if stop_markers and any(marker in line for marker in stop_markers):
                    stop_writing = True

    threading.Thread(target=_reader, daemon=True).start()


def wait_for_log_marker(log_path, marker, timeout=10, interval=0.25):
    """Wait for a marker string to appear in the log file."""
    log_path = Path(log_path)
    deadline = time.time() + timeout
    while time.time() < deadline:
        if log_path.exists():
            try:
                content = log_path.read_text(encoding="utf-8", errors="replace")
            except Exception:
                content = ""
            if marker in content:
                return True
        time.sleep(interval)
    return False


def write_ports_for_frontend(ports, project_root):
    """Write the active ports to a file the frontend can read."""
    ports_file = Path(project_root) / "frontend" / "public" / "ports.json"
    try:
        with open(ports_file, 'w') as f:
            json.dump(ports, f, indent=2)
        print(f"üìù Wrote port config to {ports_file}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not write ports.json: {e}")


def get_gpu_count():
    """Dynamically detect available NVIDIA GPUs."""
    try:
        import subprocess
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            gpus = [line.strip() for line in result.stdout.strip().split('\n') if line.strip()]
            gpu_count = len(gpus)
            if gpu_count > 0:
                print(f"Detected {gpu_count} NVIDIA GPU(s):")
                for i, gpu in enumerate(gpus):
                    print(f"  GPU {i}: {gpu}")
                return gpu_count
    except Exception as e:
        print(f"GPU detection failed: {e}")
    
    # Fallback to PyTorch detection
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"Detected {gpu_count} GPU(s) via PyTorch")
            return gpu_count
    except:
        pass
    
    print("No NVIDIA GPUs detected.")
    return 0


def launch_browser():
    """Launch browser after delay - runs in separate thread to avoid blocking startup"""
    try:
        time.sleep(25)  # Wait 25 seconds
        webbrowser.open('http://localhost:5173/')
        print("Browser launched at http://localhost:5173/")
    except Exception:
        pass  # Fail silently - don't break the app if browser launch fails


# MOVED TO MODULE LEVEL - Windows can't pickle local functions
def run_model_service(root_path):
    """Run the model service with proper GPU isolation."""
    gpu_count = get_gpu_count()
    if gpu_count >= 2:
        # If multiple GPUs, use the last GPU for main LLM inference
        model_gpu_id = str(gpu_count - 1)
        os.environ["CUDA_VISIBLE_DEVICES"] = model_gpu_id
        os.environ["GPU_ID"] = model_gpu_id
        print(f"üîí Model service will use GPU {model_gpu_id} for main LLM inference")
    elif gpu_count == 1:
        # Single GPU - use it for everything
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
        os.environ["GPU_ID"] = "0"
        print("üîí Model service will use GPU 0")
    else:
        # No GPU - run on CPU
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
        os.environ["GPU_ID"] = "-1"
        print("üîí Model service will run on CPU")
    
    if root_path not in sys.path:
        sys.path.insert(0, root_path)
    
    # Now run the actual service using subprocess without creating new console
    import subprocess
    service_script = os.path.join(root_path, "backend", "app", "model_service.py")
    subprocess.run([sys.executable, service_script], cwd=root_path)


def start_model_service(root_path):
    """Start the model service as a separate process."""
    # Check if already running
    try:
        test_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        test_socket.connect(('localhost', 5555))
        test_socket.close()
        print("Model service already running on port 5555")
        return None
    except:
        pass
    
    print("Starting model service on port 5555...")
    # Pass root_path as argument since it's at module level now
    service_process = Process(target=run_model_service, args=(root_path,))
    service_process.start()
    
    # Give it time to start (increased for better reliability)
    time.sleep(5)
    
    # Verify it started
    try:
        test_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        test_socket.settimeout(10)  # Add timeout to prevent hanging
        test_socket.connect(('localhost', 5555))
        test_socket.close()
        print("‚úÖ Model service started successfully")
        return service_process
    except Exception as e:
        print(f"‚ùå ERROR: Model service failed to start: {e}")
        print("üîÑ Attempting to terminate and restart...")
        try:
            service_process.terminate()
            service_process.wait(timeout=5)
        except:
            service_process.kill()  # Force kill if terminate doesn't work
        return None


def start_backend(host, port, gpu_id, root_path, tts_port=8002, startup_report_path=None):
    """Start backend with proper GPU assignment using subprocess with virtual environment"""
    try:
        import subprocess
        
        gpu_label = f"GPU {gpu_id}" if gpu_id >= 0 else "CPU"
        print(f"--- Starting Server Process for {gpu_label} on Port {port} ---")
        print(f"--- Using Python executable: {sys.executable} ---")
        
        # Build the command to run the backend
        cmd = [
            sys.executable, "-m", "uvicorn", 
            "backend.app.main:app",
            "--host", host,
            "--port", str(port),
            "--log-level", "info",
            "--ws-ping-interval", "300"
        ]
        
        # Set up environment variables for the subprocess
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        env["GPU_ID"] = str(gpu_id)
        env["PORT"] = str(port)
        env["TTS_PORT"] = str(tts_port)  # Tell backend where TTS is
        env["PYTHONPATH"] = root_path
        env["PYTHONUTF8"] = "1"
        env["PYTHONIOENCODING"] = "utf-8"

        backend_log_path = get_backend_log_path(port)
        env["ELOQUENT_LOG_DIR"] = str(get_log_dir(root_path))
        env["BACKEND_LOG_PATH"] = str(backend_log_path)
        env["ELOQUENT_DISABLE_BACKEND_LOG"] = "1"
        
        print(f"--- Environment 'CUDA_VISIBLE_DEVICES' set to '{gpu_id}' ---")
        print(f"--- Command: {' '.join(cmd)} ---")
        
        stdout_target = subprocess.PIPE if startup_report_path else None
        stderr_target = subprocess.STDOUT if startup_report_path else None

        popen_kwargs = {
            "env": env,
            "cwd": root_path,
            "stdout": stdout_target,
            "stderr": stderr_target,
        }
        if startup_report_path:
            popen_kwargs.update({
                "text": True,
                "encoding": "utf-8",
                "errors": "replace",
                "bufsize": 1,
            })

        # Launch the backend process (no new console - runs in same window)
        process = subprocess.Popen(cmd, **popen_kwargs)

        # Write a readiness line once the port is open
        def _mark_ready():
            if wait_for_port("127.0.0.1", port, timeout=90):
                pass
            else:
                pass

        threading.Thread(target=_mark_ready, daemon=True).start()
        
        print(f"‚úÖ Backend process started (PID: {process.pid})")
        if startup_report_path:
            stream_process_output(
                process,
                startup_report_path,
                stop_markers=["Uvicorn running on http://"]
            )
        return process, backend_log_path
        
    except Exception as e:
        print(f"FATAL ERROR: Failed to start backend server on GPU {gpu_id}, port {port}.", file=sys.stderr)
        print(e, file=sys.stderr)
        return None, None

def start_tts_service(root_path, port=8002, startup_report_path=None):
    """Start the TTS service in a new command window"""
    try:
        import subprocess
        
        # Build the command to run TTS service with port
        tts_script = os.path.join(root_path, "launch_tts.py")
        cmd = [sys.executable, tts_script]
        
        # Set TTS port via environment variable
        env = os.environ.copy()
        env["TTS_PORT"] = str(port)
        env["ELOQUENT_LOG_DIR"] = str(get_log_dir(root_path))
        env["TTS_LOG_PATH"] = str(get_tts_log_path(port))
        env["PYTHONUTF8"] = "1"
        env["PYTHONIOENCODING"] = "utf-8"
        
        print(f"--- Starting TTS Service on Port {port} ---")
        print(f"--- Command: {' '.join(cmd)} ---")
        
        stdout_target = subprocess.PIPE if startup_report_path else None
        stderr_target = subprocess.STDOUT if startup_report_path else None

        popen_kwargs = {
            "cwd": root_path,
            "env": env,
            "stdout": stdout_target,
            "stderr": stderr_target,
        }
        if startup_report_path:
            popen_kwargs.update({
                "text": True,
                "encoding": "utf-8",
                "errors": "replace",
                "bufsize": 1,
            })

        # Launch TTS service (no new console - runs in same window)
        tts_process = subprocess.Popen(cmd, **popen_kwargs)
        
        print(f"‚úÖ TTS service launched (PID: {tts_process.pid})")
        if startup_report_path:
            stream_process_output(
                tts_process,
                startup_report_path,
                stop_markers=["Uvicorn running on http://"]
            )
        return tts_process, env["TTS_LOG_PATH"]
        
    except Exception as e:
        print(f"FATAL ERROR: Failed to start TTS service on port {port}.", file=sys.stderr)
        print(e, file=sys.stderr)
        sys.exit(1)




def main():
    project_root = get_project_root()

    startup_report_path = get_startup_report_path(project_root)
    try:
        with open(startup_report_path, "w", encoding="utf-8") as f:
            f.write("===== Launcher Output =====\n")
        sys.stdout = TeeStream(sys.stdout, startup_report_path)
        sys.stderr = TeeStream(sys.stderr, startup_report_path)
    except Exception as e:
        print(f"WARNING: Could not initialize startup report tee: {e}")
    # Auto-find available ports
    port_config = get_port_config()
    backend_port = port_config["backend_port"]
    secondary_port = port_config["secondary_port"]
    tts_port = port_config["tts_port"]
    
    if backend_port is None:
        print("‚ùå FATAL: Could not find any available ports for backend!")
        sys.exit(1)
    
    print(f"üìå Using ports: backend={backend_port}, secondary={secondary_port}, tts={tts_port}")
    
    # Write ports to frontend config
    # Get local IP for remote access
    local_ip = get_local_ip()
    print(f"üåç Local Network IP: {local_ip} (Use this on mobile: http://{local_ip}:5173)")
    
    # Write ports to frontend config - use IP instead of localhost so mobile connects correctly
    write_ports_for_frontend({
        "backend": f"http://{local_ip}:{backend_port}",
        "secondary": f"http://{local_ip}:{secondary_port}",
        "tts": f"http://{local_ip}:{tts_port}"
    }, project_root)
    
    # Start browser launch timer in background thread
    browser_thread = threading.Thread(target=launch_browser, daemon=True)
    browser_thread.start()
    
    # Start the model service first
    model_service_process = start_model_service(project_root)
    
    gpu_count = get_gpu_count()
    processes = []
    backend_log_path = None
    tts_log_path = None
    secondary_backend = None
    
    if model_service_process:
        processes.append(model_service_process)
    
    if gpu_count == 0:
        print("No NVIDIA GPUs detected. Starting backend on CPU and TTS service.")
        with LOG_WRITE_LOCK:
            with open(startup_report_path, "a", encoding="utf-8") as f:
                f.write("\n===== Backend Process Output =====\n")
        main_backend, backend_log_path = start_backend(
            host="0.0.0.0",
            port=backend_port,
            gpu_id=-1,
            root_path=project_root,
            tts_port=tts_port,
            startup_report_path=startup_report_path
        )
        if main_backend:
            processes.append(main_backend)
            print(f"‚úÖ Main backend started on port {backend_port} (CPU)")
    elif gpu_count == 1:
        print(f"Found {gpu_count} NVIDIA GPU. Starting backend and TTS service on GPU 0.")
        with LOG_WRITE_LOCK:
            with open(startup_report_path, "a", encoding="utf-8") as f:
                f.write("\n===== Backend Process Output =====\n")
        main_backend, backend_log_path = start_backend(
            host="0.0.0.0",
            port=backend_port,
            gpu_id=0,
            root_path=project_root,
            tts_port=tts_port,
            startup_report_path=startup_report_path
        )
        if main_backend:
            processes.append(main_backend)
            print(f"‚úÖ Main backend started on port {backend_port} using GPU 0")
    else:
        print(f"Found {gpu_count} NVIDIA GPUs. Starting services...")
        
        # Start main backend using configured port and GPU 0
        with LOG_WRITE_LOCK:
            with open(startup_report_path, "a", encoding="utf-8") as f:
                f.write("\n===== Backend Process Output =====\n")
        main_backend, backend_log_path = start_backend(
            "0.0.0.0",
            backend_port,
            0,
            project_root,
            tts_port=tts_port,
            startup_report_path=startup_report_path
        )
        if main_backend:
            processes.append(main_backend)
            print(f"‚úÖ Main backend started on port {backend_port} using GPU 0")
        
        # For 2 GPUs: Start secondary backend for dual-model mode
        # For 3+ GPUs: Use tensor splitting across all GPUs (no secondary backend needed)
        if gpu_count == 2:
            with LOG_WRITE_LOCK:
                with open(startup_report_path, "a", encoding="utf-8") as f:
                    f.write("\n===== Secondary Backend Output =====\n")
            secondary_backend, _ = start_backend(
                "0.0.0.0",
                secondary_port,
                1,
                project_root,
                tts_port=tts_port,
                startup_report_path=startup_report_path
            )
            if secondary_backend:
                processes.append(secondary_backend)
                print(f"‚úÖ Secondary backend started on port {secondary_port} using GPU 1 (dual-model mode)")
        else:
            print(f"üí° {gpu_count} GPUs detected - using unified model mode with tensor splitting")
            print(f"   Configure tensor_split in Settings to distribute load across GPUs 0-{gpu_count-1}")
    
    # ALWAYS start TTS service (regardless of GPU count)
    with LOG_WRITE_LOCK:
        with open(startup_report_path, "a", encoding="utf-8") as f:
            f.write("\n===== TTS Process Output =====\n")
    tts_service, tts_log_path = start_tts_service(
        project_root,
        tts_port,
        startup_report_path=startup_report_path
    )
    if tts_service:
        processes.append(tts_service)
        print(f"‚úÖ TTS service launched on port {tts_port}")
    
    # Wait for all processes
    if processes:
        try:
            for p in processes:
                if hasattr(p, 'join'):  # multiprocessing.Process
                    p.join()
                elif hasattr(p, 'wait'):  # subprocess.Popen
                    p.wait()
        except KeyboardInterrupt:
            print("\nüõë Shutting down services...")
            for p in processes:
                try:
                    if hasattr(p, 'terminate'):
                        p.terminate()
                except:
                    pass


if __name__ == "__main__":
    freeze_support()
    main()
